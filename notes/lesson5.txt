Planning:

Problem solving vs planning:
Planning alone wont always work when executing since properties of environment may change. So any time env is not deterministic.
So issues are for:
-Stochastic envs
-Multiagent scenarios
-Partial observability (dont always know exactly which state we start in)


Solve by planning for belief states (number of possible states. Once action is done then you get more info and change belife state)
 and not objective world states.

Can also deal with different kinds of envs:
Deterministic and fully observable
Deterministic and partially observable
Stochastic

For deterministic you start then have an action then get a result then move again and get new result until you are in a goal state
For stochastic you start, predict state from belief states then apply and action and update your prediction based on the action.

For stochasic and partially observable you might need and inifite number of plans to be sure that the goal is achieved.
Can impliment this by doing action-> observe result then loop back and repeat or carry on. Can be done with a more complicated seearch tree.

Bounded Solution:
-Can find the solution in a bounded number of steps as long as there are no loops in the tree

Unbounded Solution:
-Can find the solution in an unbounded number of steps as long as every leaf node is a goal.



============================================================================================================================================

Classical planning:

State space: k-booleans =2^k variables
World state: complete assignment of true and false to each variable
Belief state: Complete assignment (deterministic, fully observable
	Partial assignment
	Arbitary formula
Action schema: Set of all possible actions in the schema, what need to know to apply the action and what the effect of the action will be.
	Will also come with an initial state and goal state.

Planning can be done same as problem solving with a progression search:
Go from one state to another by branching over different actions until you reach a goal. This a forward or state space search.
But with planning representation, can do more than this.

Regression (backwards) search:
Start at goal state. Goal state represents a family of things that can result in that state.
Look at action schema to figure out which actions result in that goal to work backwards.

Regression vs Progression:
Regression can result in far fewer nodes expanded than in a forward search.

